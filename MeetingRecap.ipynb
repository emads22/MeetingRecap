{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Create Meeting Minutes from an Audio File with a UI using Gradio\n",
        "\n",
        "- Start by installing the necessary dependencies required for audio processing, model loading, and the Gradio interface.\n"
      ],
      "metadata": {
        "id": "It89APiAtTUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate openai httpx==0.27.2 gradio"
      ],
      "metadata": {
        "id": "f2vvgnFpHpID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b359e63-21fc-4c4e-e679-55106560e3df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.4/321.4 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW8nl3XRFrz0"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "import torch\n",
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata, drive\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive to the Colab Environment\n",
        "\n",
        "- Mount Google Drive to enable access to files stored in your Drive directly from the Colab environment.  \n",
        "- This allows saving and loading models, files, and other resources persistently across sessions.\n"
      ],
      "metadata": {
        "id": "HTl3mcjyzIEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount Google Drive to the Colab environment, allowing access to files stored in the Drive.\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "Es9GkQ0FGCMt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6735ea0a-10f0-4fb4-a588-ec5adaa2f0e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Required Constants\n",
        "\n",
        "- Set up the necessary constants that will be used throughout the application, such as model-specific markers, token limits, and configuration values.\n"
      ],
      "metadata": {
        "id": "COWdmqYnqVlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specifies the GPT speech-to-text model to be used for audio transcription (OpenAI's Whisper model version 1)\n",
        "GPT_STT_MODEL = \"whisper-1\"\n",
        "# Specifies the path or identifier for the LLaMA model to be used for generating meeting minutes\n",
        "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "# These paths are optional and can be customized based on user preference.\n",
        "\n",
        "# DRIVE_DIR = \"/content\"  # Uncomment this line and comment the next one if you prefer to save the model in the temporary runtime session (non-persistent storage).\n",
        "DRIVE_DIR = \"/content/drive/MyDrive\"  # Path to Google Drive for persistent storage.\n",
        "DRIVE_MODELS_DIR = DRIVE_DIR + \"/my_models\"  # Directory within Google Drive to store the saved models.\n",
        "\n",
        "\n",
        "# Special marker used by the LLaMA model to indicate the end of a specific section in the generated response.\n",
        "MODEL_SPECIAL_MARKER = \"<|end_header_id|>\"\n",
        "# End-of-sequence (EOS) token used by the LLaMA model to signify the end of the entire generated response.\n",
        "MODEL_EOS = \"<|eot_id|>\"\n",
        "# Both markers will be used during post-processing to clean up the output by removing unnecessary markers.\n",
        "\n",
        "# Define the assistant's role and task instructions\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are an AI assistant designed to generate detailed meeting minutes from transcripts in markdown format. \"\n",
        "    \"Your output should include a summary, key discussion points, takeaways, and action items with assigned owners.\"\n",
        ")\n",
        "\n",
        "# Construct the user's prompt with detailed instructions and the provided transcript\n",
        "USER_PROMPT = (\n",
        "    \"The following is an excerpt from a council meeting transcript. \"\n",
        "    \"Please generate well-structured meeting minutes in markdown format, including: \"\n",
        "    \"a summary with attendees, location, and date; key discussion points; takeaways; \"\n",
        "    \"and action items with designated owners.\\n\\n\"\n",
        ")\n",
        "\n",
        "# Maximum number of tokens allowed for the model's generation to control output length and prevent exceeding limits\n",
        "MAX_TOKENS = 2000\n"
      ],
      "metadata": {
        "id": "q3D1_T0uG_Qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add Secrets to the Colab Notebook\n",
        "\n",
        "- Add your Hugging Face Hub credentials to sign in and access models.  \n",
        "- Provide your OpenAI private API key to enable access to the OpenAI services."
      ],
      "metadata": {
        "id": "Q8TtmIfkjfju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign in to HuggingFace Hub using Secrets in Colab\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "xYW8kQYtF-3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign in to OpenAI using Secrets in Colab\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "openai = OpenAI(api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "qP6OB2OeGC2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Model and Tokenizer Variables\n",
        "\n",
        "- Initialize the `model` and `tokenizer` variables by assigning them to `None`.  \n",
        "- This ensures they can be properly loaded later when needed."
      ],
      "metadata": {
        "id": "fbnNjDh8FDOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model and tokenizer variables\n",
        "tokenizer = None\n",
        "model = None"
      ],
      "metadata": {
        "id": "d0VrTdBR9i57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Whisper` OpenAI Model for Audio-to-Text Conversion\n",
        "\n",
        "- Use the `Whisper` model by OpenAI to accurately transcribe the uploaded audio file into text.  \n",
        "- This transcribed text will serve as the input for generating the meeting minutes.\n"
      ],
      "metadata": {
        "id": "3Sn7Q6OhsZNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_audio(audio_filename, progress=gr.Progress()):\n",
        "    # Update progress to indicate the transcription process has started\n",
        "    progress(0.3, desc=\"Generating meeting transcript...\")\n",
        "\n",
        "    try:\n",
        "        # Open the audio file in binary mode for reading\n",
        "        with open(audio_filename, \"rb\") as audio_file:\n",
        "            # Send the audio file to the OpenAI API for speech-to-text transcription\n",
        "            transcription = openai.audio.transcriptions.create(\n",
        "                model=GPT_STT_MODEL,          # Specify the speech-to-text model\n",
        "                file=audio_file,              # Provide the audio file to be transcribed\n",
        "                response_format=\"text\"        # Set the desired response format to plain text\n",
        "            )\n",
        "            return transcription  # Return the generated transcription result\n",
        "    except Exception as e:\n",
        "        # Handle any errors that occur during the transcription process\n",
        "        # Raise a custom exception with a detailed error message\n",
        "        raise Exception(f\"An error occurred while transcribing audio: {e}\") from e\n"
      ],
      "metadata": {
        "id": "Il9CGsB0VMfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Model and Tokenizer\n",
        "\n",
        "- If this is the first time using the runtime, load the model from the Hugging Face Hub and save it to the drive for future use (this ensures the model persists even after the runtime disconnects).  \n",
        "- Alternatively, the model can be saved in the current temporary runtime session location, but note that it will not persist after the session ends or disconnects.  \n",
        "- If the model is already saved on the drive, it will be loaded directly from there to save time.\n"
      ],
      "metadata": {
        "id": "hA1-Rs5CtGeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name, local_dir=DRIVE_MODELS_DIR):\n",
        "    # Convert the local_dir to a Path object for easier path handling\n",
        "    local_dir = Path(local_dir)\n",
        "\n",
        "    # Create a subdirectory for the model, replacing '/' in model_name with '_'\n",
        "    model_dir = local_dir / model_name.replace(\"/\", \"_\")\n",
        "\n",
        "    if model_dir.exists():  # Check if the model is already downloaded locally\n",
        "        # Load the tokenizer and model from the existing directory\n",
        "        tokenizer = AutoTokenizer.from_pretrained(str(model_dir))\n",
        "        model = AutoModelForCausalLM.from_pretrained(str(model_dir))\n",
        "\n",
        "    else:  # If the model is not available locally, download and configure it\n",
        "        # Configure the quantization settings for loading the model in 4-bit precision\n",
        "        quant_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,                   # Load the model in 4-bit precision to save memory\n",
        "            bnb_4bit_use_double_quant=True,      # Enable double quantization for better performance\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,  # Set computation data type to bfloat16\n",
        "            bnb_4bit_quant_type=\"nf4\"            # Use NF4 quantization type for improved accuracy\n",
        "        )\n",
        "\n",
        "        # Download the tokenizer with remote code support enabled\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "        # Set the padding token to the end-of-sequence (EOS) token for consistency\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        # Download and load the model with the specified quantization configuration\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name, config=quant_config)\n",
        "\n",
        "        # Save the downloaded model and tokenizer locally for future use\n",
        "        model_dir.mkdir(parents=True, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "        model.save_pretrained(model_dir)  # Save the model to the specified directory\n",
        "        tokenizer.save_pretrained(model_dir)  # Save the tokenizer to the specified directory\n",
        "\n",
        "    # Return the loaded tokenizer and model for further use\n",
        "    return tokenizer, model\n",
        "\n"
      ],
      "metadata": {
        "id": "hMwQuGd76u3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Meeting Minutes from Recorded Audio\n",
        "\n",
        "- Use the loaded model to process the transcript and produce a well-structured meeting minutes summary.\n"
      ],
      "metadata": {
        "id": "e_mJRQz_cq5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_meeting_minutes(transcription, progress=gr.Progress()):\n",
        "    # Declare tokenizer and model as global variables to allow their use and modification\n",
        "    global tokenizer, model\n",
        "\n",
        "    # Update progress to indicate that the meeting minutes generation process has started\n",
        "    progress(0.5, desc=\"Preparing to generate meeting minutes...\")\n",
        "\n",
        "    try:\n",
        "        # Check if the tokenizer or model is already loaded; if not, load or download them\n",
        "        if tokenizer is None or model is None:\n",
        "            # Load the required LLaMA tokenizer and model if missing\n",
        "            tokenizer, model = load_model(LLAMA)\n",
        "\n",
        "        # Create the message sequence following the chat template format\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},  # System role defines the assistant's task\n",
        "            {\"role\": \"user\", \"content\": USER_PROMPT + transcription}  # User input includes the transcription\n",
        "        ]\n",
        "\n",
        "        # Tokenize the input messages and prepare them for model inference on GPU\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages, return_tensors=\"pt\",  # Return tokenized input as PyTorch tensors\n",
        "            add_generation_prompt=True      # Add a generation prompt for the model\n",
        "        ).to(\"cuda\")  # Move the inputs to GPU for faster processing\n",
        "\n",
        "        # Generate the response with a specified maximum number of new tokens\n",
        "        progress(0.75, desc=\"Generating and decoding the result...\")  # Update progress\n",
        "        outputs = model.generate(inputs, max_new_tokens=MAX_TOKENS)\n",
        "\n",
        "        # Decode the generated tokens into a readable text response\n",
        "        response = tokenizer.decode(outputs[0])\n",
        "\n",
        "        # Post-process the response to clean up unwanted markers or special tokens\n",
        "        progress(0.9, desc=\"Finalizing and formatting meeting minutes...\")  # Update progress\n",
        "        response = response.split(MODEL_SPECIAL_MARKER)[-1].strip().replace(MODEL_EOS, \"\")\n",
        "\n",
        "        return response  # Return the final cleaned meeting minutes\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle any exceptions and raise a custom error with a detailed message\n",
        "        raise Exception(f\"Error generating the meeting minutes summary: {str(e)}\") from e\n"
      ],
      "metadata": {
        "id": "l5tqfQr02Ccx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process Uploaded Audio to Generate a Summarized Meeting Minutes Recap\n",
        "\n",
        "- Convert the uploaded audio file into text using the `Whisper` model.  \n",
        "- Generate a detailed and concise meeting minutes summary."
      ],
      "metadata": {
        "id": "T-eHOCYydAfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_audio(audio_file, progress=gr.Progress()):\n",
        "    # Update progress to indicate the start of audio processing\n",
        "    progress(0.1, desc=\"Start Audio Processing ...\")\n",
        "\n",
        "    # Check if no audio file was provided by the user\n",
        "    if audio_file is None:\n",
        "        return \"No audio file detected. Please upload a valid audio file to proceed.\"\n",
        "\n",
        "    # Check if the uploaded file is not in MP3 format (only MP3 files are supported)\n",
        "    elif not str(audio_file).lower().endswith(\".mp3\"):\n",
        "        return \"Unsupported file format. Please upload a valid MP3 file.\"\n",
        "\n",
        "    try:\n",
        "        # Transcribe the audio file into text using the Whisper model\n",
        "        transcription = transcribe_audio(audio_file)\n",
        "\n",
        "        # Generate meeting minutes from the transcribed text using the LLaMA model\n",
        "        output = generate_meeting_minutes(transcription)\n",
        "\n",
        "        # Update progress to indicate that the process is complete\n",
        "        progress(1.0, desc=\"Meeting Minutes Complete!\")\n",
        "        return output  # Return the generated meeting minutes\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle any exceptions that occur during processing and return an error message\n",
        "        return f\"Error processing the audio file: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "ZZL5h6nmLIAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the User Interface (UI) with Gradio\n",
        "\n",
        "- Design a simple and intuitive Gradio interface for uploading audio files and displaying the generated meeting minutes.\n",
        "- Ensure the UI supports real-time progress updates and displays the final output in markdown format.\n"
      ],
      "metadata": {
        "id": "9IjdHkJgpfJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Description displayed in the Gradio interface to guide users on how to use the app\n",
        "description = \"\"\"<div style=\"text-align: center;\">\n",
        "Upload an MP3 recording of your meeting and let MeetingRecap handle the rest.<br>\n",
        "Our AI assistant will generate a clear and accurate set of meeting minutes, including key discussions, action items, and decisions.<br>\n",
        "In just a few minutes, you'll receive a well-organized summary that saves you time and effort.\n",
        "</div><br>\"\"\"\n",
        "\n",
        "ui = gr.Interface(\n",
        "    fn=process_audio,  # Function that processes the uploaded audio file and generates meeting minutes\n",
        "    inputs=gr.Audio(type=\"filepath\", label=\"Upload Recorded MP3 File\", format=\"mp3\"),  # Audio input widget restricts input to MP3 format\n",
        "    outputs=gr.Markdown(label=\"Meeting Minutes\", min_height=60),  # Displays the output as Markdown\n",
        "    title=\"MeetingRecap\",  # Title of the Gradio interface\n",
        "    description=description,  # Description shown below the title\n",
        "    flagging_mode=\"never\"  # Disables the flagging feature in the UI\n",
        ")\n",
        "\n",
        "# Launches the Gradio interface and opens it in a browser\n",
        "ui.launch(inbrowser=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yjd2dybqpo-m",
        "outputId": "2677fb2e-4ac0-4ab4-d24c-ff233ab46056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7c9a827bef3c6564dc.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7c9a827bef3c6564dc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
            "----\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7c9a827bef3c6564dc.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7c9a827bef3c6564dc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contributing\n",
        "Contributions are welcome! Here are some ways you can contribute to the project:\n",
        "- Report bugs and issues.\n",
        "- Suggest new features or improvements.\n",
        "- Submit pull requests with bug fixes or enhancements.\n",
        "\n",
        "You can contribute to this project by visiting the [GitHub repository](https://github.com/emads22/MeetingRecap).\n",
        "\n",
        "## Author\n",
        "- **Emad**  \n",
        "  [<img src=\"https://img.shields.io/badge/GitHub-Profile-blue?logo=github\" width=\"150\">](https://github.com/emads22)\n",
        "\n",
        "## License\n",
        "This project is licensed under the MIT License, which grants permission for free use, modification, distribution, and sublicense of the code, provided that the copyright notice (attributed to [emads22](https://github.com/emads22)) and permission notice are included in all copies or substantial portions of the software. This license is permissive and allows users to utilize the code for both commercial and non-commercial purposes.\n",
        "\n",
        "Please see the [LICENSE](https://github.com/emads22/MeetingRecap/blob/main/LICENSE) file for more details.\n"
      ],
      "metadata": {
        "id": "i7VQkGygFBYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cAWsS6HuFKmf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}